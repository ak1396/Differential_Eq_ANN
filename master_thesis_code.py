# -*- coding: utf-8 -*-
"""Master Thesis Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XQahra4nqF7e1RWR0a-hBbLoeO_Jba2D

# **Master Thesis - Solving Differential Equations using Neural Network**
#### *By Akshit Gupta*
---

## Introduction

The following code was used to solve differential Equations using Neural network concepts as part of the master thesis by Akshit Gupta under the mentorship of Marius Oltean, Professor at ESADE Barcelona.
"""

# Importing relevant Libraries 

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
import math

np.random.seed(1234)

"""# ODE - Logistic Model

To model population growth using a differential equation, we first need to introduce some variables and relevant terms. The variable  ùë° . will represent time. The units of time can be hours, days, weeks, months, or even years. Any given problem must specify the units used in that particular problem. The variable  ùëÉ  will represent population. Since the population varies over time, it is understood to be a function of time. Therefore we use the notation  ùëÉ(ùë°)  for the population as a function of time. If  ùëÉ(ùë°)  is a differentiable function, then the first derivative  ùëëùëÉùëëùë°  represents the instantaneous rate of change of the population as a function of time. We use the variable  ùêæ  to denote the carrying capacity. The growth rate is represented by the variable  ùëü . Using these variables, we can define the logistic differential equation. Then the logistic differential equation is

$$ùëëùëÉ/ùëëùë°=ùëüùëÉ(1‚àíùëÉ/ùêæ)$$


For our example, we would be modelling the population growth of humans.









"""

''' This block of code approximates the Logistic Model with constant Growth rate(r) and Carrying Capacity (K)
* The growth rate 'r' is assumed a constant of 2.5% each century
* The Carrying capacity 'K' is assumed to be 12Bn
* The unit of time 't' is century (100 years). e.g. 1500,1600,1700 etc
* The unit of population is Billion or 10^9
* The condition mentioned is that population in year 2022 is 8Bn.
* Nummber of epochs will be 10,000
* In each epoch, number of training data points will be 1,000'''

# Defining Model Paramters for the neural network
p0 = 8 #Initial Condition
r = 2.5
K = 12

# Parameters
training_steps = 10000
data_points = 1000

#Initiating a basic Neural network with 1 hidden layer and 50 neurons
N = nn.Sequential(nn.Linear(1, 50), nn.Sigmoid(), nn.Linear(50,1, bias=False))

#The Approximate Solution of the Differential Equation as per I.E.Lagaris,A.LikasandD.I.Fotiadis
Psi_t = lambda t: p0 + t * N(t)

#The differential Equation to be solved
ODE = lambda Psi: r*Psi*(1-(Psi/K))

#Defining a custom loss function, which helps the Neural network to satisfy the 
def loss(x):
    x.requires_grad = True
    outputs = Psi_t(x)
    Psi_t_x = torch.autograd.grad(outputs, x, grad_outputs=torch.ones_like(outputs),
                        create_graph=True)[0]
    return  torch.mean(( Psi_t_x - ODE(outputs))**2)

#Initialising LBFGS Optimizer
optimizer = torch.optim.LBFGS(N.parameters())

#Random data points for Loss calculation in each epoch
t = torch.Tensor(np.linspace(-5, 5, data_points)[:, None])

#Approximating the function which satisfies our ODE
def closure():
    optimizer.zero_grad()
    l = loss(t)
    l.backward()
    return l

for i in range(training_steps):
    optimizer.step(closure)


#Plotting the final results
tt = np.linspace(-5, 5, 100)[:, None] #Datapoints on which the Graph will be plotted

with torch.no_grad():
    yy = Psi_t(torch.Tensor(tt)).numpy()
yt = K/(1+ (K/p0 - 1)*np.exp(-r*tt))

fig, ax = plt.subplots(dpi=100)
ax.plot(tt, yt, label='True')
ax.plot(tt, yy, '--', label='Neural network approximation')
ax.set_xlabel('$x$')
ax.set_ylabel('$Psi(x)$')
plt.legend(loc='best');

"""The assumption that there is constant Carrying Capacity and constant growth rate is not very practical. In practice, our Carrying capacity is increasing and our growth rate is decreasing. Hence, we are gonna approximate a function with time-dependent Carrying Capacity and constant growth rate.



"""

''' Since there is no analytical solution for Logistic ODE with time-dependent ODE. 
We will be verifying the results from ANN against Runge-Kutta 4th order method.

This function takes 3 arguments
* The Differential equation - 'DE'
* The step size - 'step_size
* The initial condition - y0'
'''

def RK(DE, step_size, y0):
  X = list(np.arange(0, 5, step_size))
  X_neg = list(np.arange(0, -5, -step_size))
  RK_output_pos = []
  for i in range(len(X)):
    if X[i] == 0:
      RK_output_pos.append(y0)
    else:
      K1 = step_size*DE(X[i-1],RK_output_pos[i-1])
      K2 = step_size*DE((X[i-1] + step_size/2),(RK_output_pos[i-1] + K1/2))
      K3 = step_size*DE((X[i-1] + step_size/2),(RK_output_pos[i-1] + K2/2))
      K4 = step_size*DE((X[i-1] + step_size),(RK_output_pos[i-1] + K3))
      K5 = (K1 + 2*K2 + 2*K3 + K4)/6
      y = RK_output_pos[i-1] + K5
      RK_output_pos.append(y)

  RK_output_neg = []
  for i in range(len(X_neg)):
    if X_neg[i] == 0:
      RK_output_neg.append(y0)
    else:
      K1 = -step_size*DE(X[i-1],RK_output_neg[i-1])
      K2 = -step_size*DE((X[i-1] - step_size/2),(RK_output_neg[i-1] + K1/2))
      K3 = -step_size*DE((X[i-1] - step_size/2),(RK_output_neg[i-1] + K2/2))
      K4 = -step_size*DE((X[i-1] - step_size),(RK_output_neg[i-1] + K3))
      K5 = (K1 + 2*K2 + 2*K3 + K4)/6
      y = RK_output_neg[i-1] + K5
      RK_output_neg.append(y)
  
  RK_output_neg.reverse()
  RK_output = RK_output_neg + RK_output_pos[1:]

  return RK_output

''' This block of code approximates the Logistic Model with variable Growth rate(r) and constant Carrying Capacity (K)
* The growth rate 'r' is assumed a constant of 2.5% each century
* The Carrying capacity 'K' is assumed to be 12 + 0.5*t
* The unit of time 't' is century (100 years). e.g. 1500,1600,1700 etc
* The unit of population is Billion or 10^9
* The condition mentioned is that population in year 2022 is 8Bn.
* Nummber of epochs will be 100K
* In each epoch, number of training data points will be 10K'''

# Defining Model Paramters for the neural network
p0 = 8 #Initial Condition
r = 2.5

# Parameters
training_steps2 = 1000
data_points2 = 1000

#Initiating a basic Neural network with 1 hidden layer and 50 neurons
N2 = nn.Sequential(nn.Linear(1, 50), nn.Sigmoid(), nn.Linear(50,1, bias=False))

#The Approximate Solution of the Differential Equation as per I.E.Lagaris,A.LikasandD.I.Fotiadis
Psi_t2 = lambda t: p0 + t * N2(t)

#The differential Equation to be solved
ODE2 = lambda t,Psi: r*Psi*(1-(Psi/(12 + 0.5*t)))

#Defining a custom loss function, which helps the Neural network to satisfy the 
def loss(x):
    x.requires_grad = True
    outputs2 = Psi_t2(x)
    Psi_t_x2 = torch.autograd.grad(outputs2, x, grad_outputs=torch.ones_like(outputs2),
                        create_graph=True)[0]
    return  torch.mean(( Psi_t_x2 - ODE2(x,outputs2))**2)

#Initialising LBFGS Optimizer
optimizer2 = torch.optim.LBFGS(N2.parameters())

#Random data points for Loss calculation in each epoch
t2 = torch.Tensor(np.linspace(-3,3, data_points2)[:, None])

#Approximating the function which satisfies our ODE
def closure2():
    optimizer2.zero_grad()
    l2 = loss(t2)
    l2.backward()
    return l2

for i in range(training_steps2):
    optimizer2.step(closure2)


#Plotting the final results
tt2 = np.linspace(-5, 5, 999)[:, None] #Datapoints on which the Graph will be plotted

with torch.no_grad():
    yy2 = Psi_t2(torch.Tensor(tt2)).numpy()
yt2 = RK(ODE2, 0.01, p0)

fig, ax = plt.subplots(dpi=100)
ax.plot(tt2, yt2, label='True')
ax.plot(tt2, yy2, '--', label='Neural network approximation')
ax.set_xlabel('$x$')
ax.set_ylabel('$Psi(x)$')
plt.legend(loc='best');

"""# PDE

In this section, we will demonstrate, how to solve PDE using Artificial Neural Network.
"""

#Initiating Neural Network
N3 = nn.Sequential(nn.Linear(2, 50), nn.Sigmoid(), nn.Linear(50,1, bias=False))


#Approx Neural Network
Approx_psi = lambda x,t: (1-(t**2))*np.sin(np.pi*torch.detach(x)) + x*(1-x)*(t**2)*N3(torch.tensor([[x,t]]))

#Loss function
def loss(X,T):

    X.requires_grad = True
    T.requires_grad = True

    outputs = torch.empty(len(X),1)
    i = 0
    for x,t in zip(X,T):
      outputs[i] = Approx_psi(x,t)
      i = i +1

    grad_Psi_x = torch.autograd.grad(outputs, X, grad_outputs=torch.ones_like(outputs),create_graph=True)[0]
    grad_Psi2_x = torch.autograd.grad(grad_Psi_x, X, grad_outputs=torch.ones_like(grad_Psi_x),create_graph=True)[0]

    grad_Psi_t = torch.autograd.grad(outputs, T, grad_outputs=torch.ones_like(outputs),create_graph=True)[0]
    grad_Psi2_t = torch.autograd.grad(grad_Psi_x, T, grad_outputs=torch.ones_like(grad_Psi_x),create_graph=True)[0]


    return  torch.mean((grad_Psi2_x - grad_Psi2_t)**2)


optimizer = torch.optim.LBFGS(N3.parameters())
t3 = torch.Tensor(np.linspace(0, 1, 1000)[:, None])
x = torch.Tensor(np.linspace(0, 1, 1000)[:, None])

def closure():
    optimizer.zero_grad()
    l = loss(x,t3)
    l.backward()
    return l

for i in range(1000):
    optimizer.step(closure)